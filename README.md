# AI-Powered-Bank-Customer-Churn-Prediction-Revenue-Impact-Analysis-System


## Introduction
In today‚Äôs highly competitive financial industry, customer retention has become just as critical as customer acquisition. Banks invest heavily in onboarding new customers, yet many customers eventually disengage and close their accounts ‚Äî a phenomenon known as churn. Losing a customer not only reduces revenue but also increases the cost burden associated with continuously replacing lost customers.
Advancements in Artificial Intelligence (AI) and Machine Learning (ML) now provide banks with the ability to proactively identify customers who are at risk of churn, understand the drivers behind their decision, and take timely actions to retain them. This project develops an intelligent churn prediction system that uses customer behavior, financial activities, and engagement metrics to forecast churn probability and revenue impact. The outcome empowering banks to focus resources on high-value at-risk customers who are most likely to respond to retention campaigns.

## Statement of Problem

Banks often struggle with:

‚Ä¢	Difficulty in identifying churn-prone customers before they leave

‚Ä¢	Lack of insight into why customers churn

‚Ä¢	High marketing and retention costs due to poor targeting

‚Ä¢	Loss of revenue, customer lifetime value, and market share

‚Ä¢	Manual decision-making with limited predictive intelligence

Therefore, a data-driven solution is required to predict churn early, quantify business impact, and drive smart retention strategy.

## Project Objectives
This project aims to:

i.	Build a machine learning model that predicts the probability of customer churn accurately.

ii.	Identify the key behavioral and financial factors contributing to churn.

iii.	Estimate the financial impact of each potential churn case, enabling value-based prioritization.

iv.	Provide actionable insights and segmentation for targeted retention strategies.

v.	Deploy a prototype decision-support tool to assist bank teams in monitoring and reducing churn proactively.




Projected Financial Impact and Savings
The AI-Powered Bank Customer Churn Prediction & Revenue Impact Analysis System targets a critical area of business revenue: Customer Lifetime Value (CLV). By moving the bank from reactive (after the customer leaves) to proactive retention, the system delivers substantial and measurable financial benefits.

1. The Cost of Customer Churn

The financial burden of customer churn is often underestimated, encompassing more than just lost monthly fees.

| Metric | Industry Figure (Approximate) | Business Implication |
|--------|------------------------------|---------------------|
| Cost of Acquisition vs. Retention | Acquiring a new customer can cost 5 to 25 times more than retaining an existing one. | The project's success immediately translates into significant cost avoidance. |
| Typical Annual Churn Rate (Banking) | Varies, but often falls between 10% to 20% annually. | If a bank has 1 million customers, losing 15% means 150,000 customers/year and massive CLV loss. |
| Customer Lifetime Value (CLV) | Can range from $1,000 to over $5,000 per average bank customer, depending on product holdings. | Each lost customer represents a significant, long-term asset value. |


2. Projected Savings through Proactive Retention
   
The savings generated by the AI system come from two main sources: Recovered Revenue (Preventing CLV Loss) and Optimized Marketing Spend.

A. Recovered Customer Lifetime Value (CLV)
The project's central value is enabling the retention of high-value customers who were otherwise going to leave.

‚Ä¢	Target Retention Rate: Even a modest improvement in retention can have a huge impact. For example, increasing retention rates by just 5% can boost profits by 25% to 95% (Bain & Company).

‚Ä¢	Targeted Intervention: By identifying customers with a high probability of churn AND a high CLV (Phase 5: Revenue Impact Analysis), the bank can target retention efforts with maximum efficiency.

Illustrative Savings Calculation:

Assume a bank has 500,000 customers, an annual churn rate of 15% (75,000 churners), and an average CLV of $2,000.

| Scenario | Calculation | Financial Impact |
|----------|-------------|-----------------|
| Total Annual CLV Loss (Baseline) | 75,000 churners * $2,000/customer | $150,000,000 |
| AI Retention Goal | Aim to save just 10% of the high-value churners identified by the model (7,500 customers). | 7,500 customers saved * $2,000/customer |
| Projected Annual Savings (CLV Recovery) |  | $15,000,000 |



This figure represents the direct revenue preserved by the system.

B. Optimized Marketing and Retention Spend
The model ensures that valuable resources (discounts, special offers, personalized outreach) are only used where they have the highest probability of success.

‚Ä¢	Reduced Retention Waste: Instead of offering expensive incentives to all customers or those not actually at risk, the model directs campaigns to the Top 5%-10% most likely to churn. This can reduce unnecessary spending by 50% or more.

‚Ä¢	Increased Campaign ROI: By understanding the key factors contributing to churn (Objective ii), the bank can tailor retention offers (e.g., "low-interest credit card" vs. "fee waiver") to the specific reason the customer is leaving, significantly increasing the campaign's success rate.

‚Ä¢	Efficiency in Customer Service: Proactive outreach can resolve issues before they escalate, reducing the need for costly, reactive complaint resolution later in the customer life cycle.
The AI-Powered Bank Customer Churn Prediction System is not just a technology project; it is a revenue enhancement initiative designed to protect the bank's most valuable asset, its existing profitable customers.

## Tech Stack
This solution will be developed using:

‚Ä¢	Python for data processing and machine learning

‚Ä¢	Pandas & NumPy for data manipulation

‚Ä¢	Scikit-Learn for model development and evaluation

‚Ä¢	Matplotlib, Seaborn, and Plotly for insights and visual analytics

‚Ä¢	Optional enhancements:

  - Streamlit for an operational churn analysis dashboard
  - SQLite for scalable structured data storage



## Project Implementation Steps
### Phase 1 ‚Äî Data Ingestion & Database Setup

In real-world banking environments, customer data is typically stored in secure databases rather than spreadsheets. To replicate an enterprise-ready analytics workflow, the first step is to migrate the Excel dataset into a structured SQL database. This ensures scalable storage, fast querying, better data governance, and seamless integration with future analytics and dashboards.
In this phase, we simulate how a bank would operationally manage its customer data by transferring the Excel file (raw offline export) into an SQLite database. This forms the foundation for churn prediction, revenue impact analysis, and dashboard reporting in subsequent phases.

#### Step 1: Generate an Excel to mimic a real bank db

Below is the link for 10,000 bank customers and their transaction 

https://github.com/Ogbunugafor-Philip/AI-Powered-Bank-Customer-Churn-Prediction-Revenue-Impact-Analysis-System/blob/main/bank_full_customer_db.xlsx

#### Step 2: Create structured tables in SQLite to accommodate the excel

‚Ä¢	Open your terminal and create a fresh new database file called customer_analytics.db. Run the command
```
sqlite3 customer_analytics.db
```
<img width="939" height="197" alt="image" src="https://github.com/user-attachments/assets/9ba933c4-377e-4f1e-801a-c484dabdfb09" />


‚Ä¢	Create the customer_analytics.db table. Run;

```
CREATE TABLE bank_customers (
    customer_id TEXT PRIMARY KEY,
    age INTEGER,
    gender TEXT,
    region TEXT,
    branch_id TEXT,
    join_date TEXT,
    tenure_years REAL,
    account_type TEXT,
    monthly_salary REAL,
    current_balance REAL,
    avg_monthly_balance REAL,
    products_owned INTEGER,
    credit_score INTEGER,
    loan_amount REAL,
    loan_status TEXT,
    complaints_last_6m INTEGER,
    mobile_app_logins_30d INTEGER,
    card_transactions_30d INTEGER,
    online_txn_30d INTEGER,
    is_salary_account INTEGER,
    is_vip INTEGER,
    last_txn_days_ago INTEGER,
    engagement_score REAL,
    risk_score REAL,
    estimated_clv REAL
);
```

‚Ä¢	Inserting 10,000 Excel rows cannot be done directly inside SQLite because SQLite does not know how to read Excel (.xlsx) files, so we will use Python as the bridge. Exit the SQL;
```
.exit
```

‚Ä¢	Run the below in your local host to import the excel from your local host to your cloud server;
```
scp C:\Users\YourName\Desktop\bank_full_customer_db.xlsx root@YOUR_SERVER_IP:/root/projects/bank_churn/
``` 
<img width="975" height="244" alt="image" src="https://github.com/user-attachments/assets/775e24f5-0a5b-4157-bafc-f4bd7371553a" />


‚Ä¢	Install openpyxl in the terminal. This will allow Pandas to read .xlsx files properly. Run;
```
pip install openpyxl
``` 

‚Ä¢	Install Pandas. Run;
```
pip install pandas
```
 <img width="975" height="255" alt="image" src="https://github.com/user-attachments/assets/5a622567-68bf-4fc8-a1aa-b86fa41e3462" />


‚Ä¢	Log onto you Python terminal; Run
```
python3
```

‚Ä¢	Then run these EXACT lines
```
import pandas as pd
df = pd.read_excel("bank_full_customer_db.xlsx")
df.head()
``` 
<img width="975" height="204" alt="image" src="https://github.com/user-attachments/assets/3c3bf61c-fdc9-4245-bd23-769cf40d974a" />


#### Step 3: Validate inserts with row counts

‚Ä¢	Run these 2 commands one by one to validate that our 10,000 customers have been exported to our db
```
sqlite3 customer_analytics.db
SELECT COUNT(*) FROM bank_customers;
```
<img width="975" height="150" alt="image" src="https://github.com/user-attachments/assets/788d5706-6740-4146-9c62-208b3d9fc1a8" />


#### Step 4: Create a table for revenue analysis impact and insight and strategy

‚Ä¢	Still in your SQLite, paste the below
```
CREATE TABLE IF NOT EXISTS revenue_insights (
    customer_id TEXT PRIMARY KEY,
    churn_probability REAL,
    estimated_clv REAL,
    risk_tier TEXT,
    recommended_action TEXT,
    FOREIGN KEY (customer_id) REFERENCES bank_customers(customer_id)
);
 
```
<img width="975" height="236" alt="image" src="https://github.com/user-attachments/assets/34544298-c5e2-426a-8e58-d586857f54f7" />


### Phase 2 ‚Äî Model Development (Churn Risk Scoring)
The bank currently has no system to proactively identify customers who are likely to churn. Customer exits are typically discovered after they stop transacting or close their accounts, by which time revenue has already been lost.
Phase 2 introduces AI-driven churn risk prediction, enabling the bank to forecast which customers are most likely to disengage based on their behavior, financial activity, and overall engagement. This predictive intelligence allows the bank to move from a reactive customer management strategy to a proactive retention strategy.
In this phase, we transform raw customer data into actionable machine learning outputs. We engineer a churn label using business rules that mimic real churn behaviors (e.g., long inactivity, falling engagement, low transactions). Then we train classification models such as Logistic Regression and Random Forest/XGBoost to estimate churn probability for every customer.
The output of this phase is a churn score (0.00 ‚Äì 1.00) that quantifies the likelihood that each customer will leave. These predictions are then stored back into the bank‚Äôs SQLite analytics database to support later revenue risk assessment and precision retention strategies.
This phase builds the core intelligence behind the entire churn prevention system, helping the bank protect customer lifetime value, reduce churn rate, and grow profitability through data-driven decision-making.

#### Step 1 ‚Äî Engineer a churn label (create target column)

Business rules to identify churners (e.g., last_txn_days_ago > threshold) and generate new column: churn_label (0 = active, 1 = churned)

‚Ä¢	Create a folder named src then create a file named 01_engineer_churn_label.py in it

‚Ä¢	Paste the below in the just created file
```
import sqlite3
import pandas as pd

# Connect to the SQLite database
conn = sqlite3.connect("../database/customer_analytics.db")

# Load the customer table
df = pd.read_sql_query("SELECT * FROM bank_customers", conn)

# Create churn label: 1 = churned, 0 = active
df["churn_label"] = df["last_txn_days_ago"].apply(lambda x: 1 if x > 90 else 0)

# Save back to the same table
df.to_sql("bank_customers", conn, if_exists="replace", index=False)

print("‚úÖ churn_label created and saved to bank_customers table.")
print(df[["customer_id", "last_txn_days_ago", "churn_label"]].head())

conn.close()
```

‚Ä¢	In terminal, run the below to check that we are good
```
cd src
python3 01_engineer_churn_label.py
``` 
<img width="850" height="250" alt="image" src="https://github.com/user-attachments/assets/60497cc0-728d-44de-bffb-f81c45ad800c" />


#### Step 2 ‚Äî Select features & preprocess

Drop irrelevant columns (customer_id, join_date‚Ä¶), handle missing values, encode categorical features and scale or normalize numeric features if needed

‚Ä¢	Install scikit learn, run;
```
pip install scikit-learn
```
 <img width="975" height="318" alt="image" src="https://github.com/user-attachments/assets/41a11bac-8da9-49a3-af67-80ddf271874a" />


‚Ä¢	In the src folder, create a file named 02_preprocess_features.py and paste the below in it
```
import os
import sqlite3
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import joblib

# -----------------------------
# 1. Load data from SQLite
# -----------------------------
DB_PATH = "../database/customer_analytics.db"
TABLE_NAME = "bank_customers"
TARGET_COL = "churn_label"

print("üîó Connecting to SQLite database...")
conn = sqlite3.connect(DB_PATH)

print(f"üì• Loading table '{TABLE_NAME}' from {DB_PATH} ...")
df = pd.read_sql_query(f"SELECT * FROM {TABLE_NAME}", conn)
conn.close()

print(f"‚úÖ Loaded {df.shape[0]} rows and {df.shape[1]} columns.")

# -----------------------------
# 2. Basic sanity checks
# -----------------------------
if TARGET_COL not in df.columns:
    raise ValueError(
        f"Target column '{TARGET_COL}' not found. "
        "Make sure you have run 01_engineer_churn_label.py first."
    )

# Columns that are IDs or timestamps that we don't want as features
drop_cols = ["customer_id", "join_date"]

for col in drop_cols:
    if col not in df.columns:
        print(f"‚ö†Ô∏è Warning: '{col}' not found in dataframe. Skipping drop for this column.")

drop_cols = [c for c in drop_cols if c in df.columns]

# -----------------------------
# 3. Split into X (features) and y (target)
# -----------------------------
print("üìä Preparing features (X) and target (y)...")

y = df[TARGET_COL].astype(int)
X = df.drop(columns=[TARGET_COL] + drop_cols)

print(f"Features shape before encoding: {X.shape}")
print(f"Target shape: {y.shape}")

# -----------------------------
# 4. Identify numeric & categorical columns
# -----------------------------
numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = [col for col in X.columns if col not in numeric_features]

print(f"üî¢ Numeric features ({len(numeric_features)}): {numeric_features}")
print(f"üî† Categorical features ({len(categorical_features)}): {categorical_features}")

if len(numeric_features) == 0:
    raise ValueError("No numeric features detected. Check your data types.")

# -----------------------------
# 5. Build preprocessing pipelines
# -----------------------------
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ]
)

# Keep it as a pipeline so Step 3 can reuse the same preprocessing
full_preprocess_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor)
])

# -----------------------------
# 6. Train-test split
# -----------------------------
print("‚úÇÔ∏è Splitting into train and test sets (80/20, stratified)...")

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print(f"Train shape: {X_train.shape}, Test shape: {X_test.shape}")

# -----------------------------
# 7. Fit pipeline on training data
# -----------------------------
print("‚öôÔ∏è Fitting preprocessing pipeline on training data...")
full_preprocess_pipeline.fit(X_train, y_train)

# -----------------------------
# 8. Transform train & test
# -----------------------------
print("üîÑ Transforming train and test data...")
X_train_processed = full_preprocess_pipeline.transform(X_train)
X_test_processed = full_preprocess_pipeline.transform(X_test)

print("‚úÖ Preprocessing complete.")
print(f"Processed X_train shape: {X_train_processed.shape}")
print(f"Processed X_test shape: {X_test_processed.shape}")

# -----------------------------
# 9. Save artifacts for Step 3
# -----------------------------
ARTIFACT_DIR = "../artifacts"
os.makedirs(ARTIFACT_DIR, exist_ok=True)

pipeline_path = os.path.join(ARTIFACT_DIR, "preprocess_pipeline.joblib")
data_path = os.path.join(ARTIFACT_DIR, "churn_ml_ready_data.joblib")

print(f"üíæ Saving preprocessing pipeline to: {pipeline_path}")
joblib.dump(full_preprocess_pipeline, pipeline_path)

print(f"üíæ Saving processed data to: {data_path}")
joblib.dump(
    {
        "X_train": X_train_processed,
        "X_test": X_test_processed,
        "y_train": y_train,
        "y_test": y_test,
        "feature_columns": X.columns.tolist(),
        "numeric_features": numeric_features,
        "categorical_features": categorical_features,
    },
    data_path
)

print("üéâ Step 2 completed successfully: features selected & preprocessed.")
```

‚Ä¢	In terminal, run the below to check that we are good
```
cd src
python3 02_preprocess_features.py
```
<img width="975" height="529" alt="image" src="https://github.com/user-attachments/assets/5eaa29e3-4cc5-41b2-9d4a-79d0b31a4495" />

 

#### Step 3 ‚Äî Train ML classification models

We would create our baseline Model: Logistic Regression, improved Models: Random Forest and compare performance results

‚Ä¢	In the src folder, create a file named 03_train_models.py and paste the below in it
```
import os
import joblib
import sqlite3
import numpy as np
import pandas as pd

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score
)

# ----------------------------------------
# Load Preprocessed Train/Test Data
# ----------------------------------------
ARTIFACT_DIR = "../artifacts"
data_path = os.path.join(ARTIFACT_DIR, "churn_ml_ready_data.joblib")
pipeline_path = os.path.join(ARTIFACT_DIR, "preprocess_pipeline.joblib")

print("üì• Loading preprocessed data...")
data = joblib.load(data_path)
X_train = data["X_train"]
X_test = data["X_test"]
y_train = data["y_train"]
y_test = data["y_test"]

print("üì• Loading preprocessing pipeline...")
preprocess_pipeline = joblib.load(pipeline_path)

# ----------------------------------------
# Initialize Models
# ----------------------------------------
models = {
    "Logistic Regression": LogisticRegression(max_iter=500),
    "Random Forest": RandomForestClassifier(
        n_estimators=200,
        max_depth=12,
        random_state=42,
        class_weight="balanced"
    )
}

model_results = {}

# ----------------------------------------
# Train, Predict & Evaluate Models
# ----------------------------------------
print("\nüöÄ Training & Evaluating Models...")
for model_name, model in models.items():
    print(f"\nü§ñ Training: {model_name}")
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_proba)

    model_results[model_name] = {
        "model": model,
        "accuracy": acc,
        "precision": prec,
        "recall": rec,
        "f1_score": f1,
        "roc_auc": auc
    }

    print(f"üìä {model_name} Results:")
    print(f"‚úî Accuracy: {acc:.4f}")
    print(f"‚úî Precision: {prec:.4f}")
    print(f"‚úî Recall: {rec:.4f}")
    print(f"‚úî F1 Score: {f1:.4f}")
    print(f"‚úî ROC-AUC: {auc:.4f}")

# ----------------------------------------
# Pick Best Model from ROC-AUC Score
# ----------------------------------------
best_model_name = max(model_results, key=lambda x: model_results[x]["roc_auc"])
best_model = model_results[best_model_name]["model"]

best_model_path = os.path.join(ARTIFACT_DIR, "best_model.joblib")
joblib.dump(best_model, best_model_path)

print("\nüèÜ Best Model:", best_model_name)
print("üíæ Saved best model to:", best_model_path)

# Save performance summary for Phase 3 reporting
results_path = os.path.join(ARTIFACT_DIR, "model_performance_results.joblib")
joblib.dump(model_results, results_path)

print("üíæ Model performance summary saved!")

# ----------------------------------------
# Update SQLite Database Structure
# ----------------------------------------
print("\nüóÑÔ∏è Preparing database for probability storage...")
DB_PATH = "../database/customer_analytics.db"
conn = sqlite3.connect(DB_PATH)
cursor = conn.cursor()

# Add columns only if missing
print("üîç Updating bank_customers structure...")

# Check existing table structure first
cursor.execute("PRAGMA table_info(bank_customers)")
existing_columns = [column[1] for column in cursor.fetchall()]

if "churn_probability" not in existing_columns:
    cursor.execute("ALTER TABLE bank_customers ADD COLUMN churn_probability REAL")
    print("‚úî Added churn_probability column")

if "risk_tier" not in existing_columns:
    cursor.execute("ALTER TABLE bank_customers ADD COLUMN risk_tier TEXT")
    print("‚úî Added risk_tier column")

# Create revenue_insights table if not exists
cursor.execute("""
CREATE TABLE IF NOT EXISTS revenue_insights (
    customer_id TEXT PRIMARY KEY,
    churn_probability REAL,
    estimated_clv REAL,
    risk_tier TEXT,
    FOREIGN KEY (customer_id) REFERENCES bank_customers(customer_id)
)
""")

print("‚úî revenue_insights table ready for inserts")

conn.commit()
conn.close()

print("\nüéØ Step 3 completed successfully!")
print("‚û°Ô∏è Next: Predict churn probabilities & populate database (Step 4)")
```

‚Ä¢	In terminal, run the below to check that we are good
```
cd src
python3 03_train_models.py
```
<img width="975" height="560" alt="image" src="https://github.com/user-attachments/assets/1e051735-5af7-49d2-8de5-251bfb4638fa" />
 

#### Step 4 ‚Äî Produce churn probability scores
We now predict probability for every customer and add new column: churn_probability

‚Ä¢	In the src folder, create a file named 04_predict_and_store.py and paste the below in it
```
import sqlite3
import pandas as pd
import numpy as np
import joblib
import os

# ----------------------------------------
# Load model + preprocessing pipeline
# ----------------------------------------
ARTIFACT_DIR = "../artifacts"
BEST_MODEL_PATH = os.path.join(ARTIFACT_DIR, "best_model.joblib")
PREPROCESS_PATH = os.path.join(ARTIFACT_DIR, "preprocess_pipeline.joblib")

print("üì• Loading best model...")
model = joblib.load(BEST_MODEL_PATH)

print("üì• Loading preprocess pipeline...")
preprocess_pipeline = joblib.load(PREPROCESS_PATH)

# ----------------------------------------
# Load full dataset from SQLite
# ----------------------------------------
DB_PATH = "../database/customer_analytics.db"

print("üîó Connecting to SQLite...")
conn = sqlite3.connect(DB_PATH)

print("üì• Loading bank_customers table...")
df = pd.read_sql_query("SELECT * FROM bank_customers", conn)

if "churn_label" not in df.columns:
    raise ValueError("‚ùå churn_label not found. Ensure Step 1 & 2 are completed correctly.")

# ----------------------------------------
# Prepare features
# ----------------------------------------
drop_cols = ["customer_id", "join_date"]
X = df.drop(columns=[c for c in drop_cols if c in df.columns])

# ----------------------------------------
# Predict churn probability
# ----------------------------------------
print("ü§ñ Predicting churn probabilities...")
X_processed = preprocess_pipeline.transform(X)
df["churn_probability"] = model.predict_proba(X_processed)[:, 1]

# ----------------------------------------
# Create risk tiers for business relevance
# ----------------------------------------
def categorize_risk(prob):
    if prob >= 0.70:
        return "High"
    elif prob >= 0.40:
        return "Medium"
    else:
        return "Low"

df["risk_tier"] = df["churn_probability"].apply(categorize_risk)

print("‚ö†Ô∏è Risk Tier Distribution:")
print(df["risk_tier"].value_counts())

# ----------------------------------------
# Update customer table with predictions
# ----------------------------------------
print("üíæ Updating bank_customers with predictions...")
df.to_sql("bank_customers", conn, if_exists="replace", index=False)

# ----------------------------------------
# Update revenue_insights table too
# ----------------------------------------
print("üíæ Updating revenue_insights...")
revenue_df = df[["customer_id", "churn_probability", "estimated_clv", "risk_tier"]]

revenue_df.to_sql("revenue_insights", conn, if_exists="replace", index=False)

conn.close()

print("\nüéâ Step 4 complete: churn probabilities stored & actionable!")
print("‚û°Ô∏è Proceed to Step 5: Revenue Impact Insights & Retention Strategy")
```

‚Ä¢	Run the below command
```
Python3 04_predict_and_store.py
``` 
<img width="975" height="479" alt="image" src="https://github.com/user-attachments/assets/3d806ea5-073a-44aa-b2cf-45e3ec1a38ad" />

### Phase 3 ‚Äî Model Evaluation & Explainability
After training the churn prediction model, it is critical not only to measure how accurately it predicts customer churn, but also to understand why those predictions are made. In real banking environments, model transparency is essential because management must be confident that recommendations are based on valid customer behavior and financial insights.
This phase focuses on evaluating the model‚Äôs performance using industry-standard metrics and then revealing the key churn drivers behind the predictions. To achieve this, the following analyses will be conducted:
   - ROC-AUC Curve: Measures the model‚Äôs ability to distinguish churn-risk customers from loyal customers across all risk thresholds.
   - Feature Importance Analysis: Shows which customer behaviors and attributes have the strongest influence on churn outcomes.
   - SHAP Explainability: Provides customer-level interpretation, explaining exactly why the model labeled each customer as high- or low-risk.

By combining performance evaluation with explainability, this phase ensures that the AI system is both accurate and trustworthy, empowering bank teams to confidently take targeted retention actions that protect revenue and enhance customer lifetime value.


#### Step 1 ‚Äî Load Best Model and Test Data
In this step, we retrieve the trained best model and the processed test dataset so that we can evaluate how well the model performs on unseen customer records.

‚Ä¢	In the evaluation folder, create a file named eval_01_load_artifacts.py and paste the below in it
```
# Step 1 ‚Äî Load Best Model and Test Data

import os
import joblib

print("üì• Loading best churn prediction model...")

# We are now inside src/evaluation, so adjust the path:
ARTIFACT_DIR = "../../artifacts"

BEST_MODEL_PATH = os.path.join(ARTIFACT_DIR, "best_model.joblib")
DATA_PATH = os.path.join(ARTIFACT_DIR, "churn_ml_ready_data.joblib")

model = joblib.load(BEST_MODEL_PATH)

print("üì• Loading test data from artifacts...")
data = joblib.load(DATA_PATH)

X_test = data["X_test"]
y_test = data["y_test"]

print("üîç Data loaded successfully!")
print(f"Test set size: {X_test.shape[0]} customers")
```

‚Ä¢	Run the script;
```
python3 eval_01_load_artifacts.py
```
<img width="950" height="209" alt="image" src="https://github.com/user-attachments/assets/5212ff15-0211-4ff1-afef-44663a360e9f" />
 

#### Step 2 ‚Äî Performance Evaluation Metrics

In this step, we evaluate how well the churn prediction model performs on unseen test data. We compute key classification metrics including Accuracy, Precision, Recall, F1 score, and ROC-AUC to ensure the model predictions are reliable for business decision-making.

‚Ä¢	In the evaluation folder, create a file named eval_02_classification_metrics.py and paste the below in it
```
# Step 2 ‚Äî Performance Evaluation Metrics

import os
import joblib
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, classification_report
)

print("\nüìä Phase 3 ‚Äî Step 2: Performance Evaluation Metrics")

# Load artifacts
ARTIFACT_DIR = "../../artifacts"
BEST_MODEL_PATH = os.path.join(ARTIFACT_DIR, "best_model.joblib")
DATA_PATH = os.path.join(ARTIFACT_DIR, "churn_ml_ready_data.joblib")

model = joblib.load(BEST_MODEL_PATH)
data = joblib.load(DATA_PATH)

X_test = data["X_test"]
y_test = data["y_test"]

# Predictions
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

# Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_proba)

print(f"‚úî Accuracy: {accuracy:.4f}")
print(f"‚úî Precision: {precision:.4f}")
print(f"‚úî Recall: {recall:.4f}")
print(f"‚úî F1 Score: {f1:.4f}")
print(f"‚úî ROC-AUC: {roc_auc:.4f}")

print("\nüìå Classification Report:")
print(classification_report(y_test, y_pred))
```

‚Ä¢	Run the script;
```
python3 eval_02_classification_metrics.py
``` 
<img width="964" height="330" alt="image" src="https://github.com/user-attachments/assets/8b964cf3-9472-43dc-9a59-a25f5696f314" />


#### Step 3 ‚Äî ROC Curve & Confusion Matrix Visuals
In this step, we visualize the model‚Äôs performance using ROC-AUC and Confusion Matrix charts. These enable stakeholders to quickly assess model reliability and detect any misclassification pattern.

‚Ä¢	Install Seaborn, run;
```
pip install seaborn
```
<img width="975" height="340" alt="image" src="https://github.com/user-attachments/assets/55e5c2af-8762-4f00-9fea-24ec868709d4" />



‚Ä¢	In the evaluation folder, create a file named eval_03_roc_confusion_matrix.py and paste the below in it
```
import os
import joblib
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix
import seaborn as sns
import numpy as np

print("\nüìà Phase 3 ‚Äî Step 3: ROC Curve & Confusion Matrix")

ARTIFACT_DIR = "../../artifacts"
BEST_MODEL_PATH = os.path.join(ARTIFACT_DIR, "best_model.joblib")
DATA_PATH = os.path.join(ARTIFACT_DIR, "churn_ml_ready_data.joblib")

# Create a directory to store evaluation images
OUTPUT_DIR = "../../evaluation_results"
os.makedirs(OUTPUT_DIR, exist_ok=True)

model = joblib.load(BEST_MODEL_PATH)
data = joblib.load(DATA_PATH)

X_test = data["X_test"]
y_test = data["y_test"]

y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

# ---- ROC Curve ----
fpr, tpr, _ = roc_curve(y_test, y_proba)
auc = roc_auc_score(y_test, y_proba)

plt.figure()
plt.plot(fpr, tpr, label=f"AUC = {auc:.4f}")
plt.plot([0, 1], [0, 1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC-AUC Curve - Churn Prediction Model")
plt.legend()
roc_path = os.path.join(OUTPUT_DIR, "roc_curve.png")
plt.savefig(roc_path)
plt.close()
print(f"‚úî ROC Curve saved: {roc_path}")

# ---- Confusion Matrix ----
cm = confusion_matrix(y_test, y_pred)

plt.figure()
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - Churn Prediction Model")
plt.xlabel("Predicted")
plt.ylabel("Actual")
conf_matrix_path = os.path.join(OUTPUT_DIR, "confusion_matrix.png")
plt.savefig(conf_matrix_path)
plt.close()
print(f"‚úî Confusion Matrix saved: {conf_matrix_path}")

print("\nüéØ Step 3 completed successfully! Charts saved to evaluation_results folder.")
```

‚Ä¢	Run the script;
```
python3 eval_03_roc_confusion_matrix.py
``` 
<img width="975" height="305" alt="image" src="https://github.com/user-attachments/assets/980b5ed0-cf03-467a-ac8f-40f13055b20f" />


### Step 4 ‚Äî Feature Importance Analysis
This step identifies the behavioral and financial factors that most strongly influence a customer's likelihood to churn. These insights allow the bank to target retention actions where they matter most ‚Äî for example, improving engagement or addressing complaints before the customer leaves.

‚Ä¢	In the evaluation folder, create a file named eval_04_feature_importance.py and paste the below in it
```
# Step 4 ‚Äî Feature Importance Analysis (Corrected with Encoded Feature Names)

import os
import joblib
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder

print("\nüìä Phase 3 ‚Äî Step 4: Feature Importance Analysis")

# Load artifacts
ARTIFACT_DIR = "../../artifacts"
BEST_MODEL_PATH = os.path.join(ARTIFACT_DIR, "best_model.joblib")
DATA_PATH = os.path.join(ARTIFACT_DIR, "churn_ml_ready_data.joblib")
PIPELINE_PATH = os.path.join(ARTIFACT_DIR, "preprocess_pipeline.joblib")

OUTPUT_DIR = "../../evaluation_results"
os.makedirs(OUTPUT_DIR, exist_ok=True)

model = joblib.load(BEST_MODEL_PATH)
data = joblib.load(DATA_PATH)
pipeline = joblib.load(PIPELINE_PATH)

numeric_features = data["numeric_features"]
categorical_features = data["categorical_features"]

# Extract encoded category names
encoder = pipeline["preprocessor"].named_transformers_["cat"].named_steps["encoder"]
encoded_cat_features = encoder.get_feature_names_out(categorical_features).tolist()

# Combine numeric + encoded categorical
feature_columns = numeric_features + encoded_cat_features

# Check matching sizes
assert len(feature_columns) == len(model.feature_importances_), "Feature mismatch!"

# Create DataFrame
feature_importance_df = pd.DataFrame({
    "feature": feature_columns,
    "importance": model.feature_importances_
}).sort_values(by="importance", ascending=False)

print("\nüîù Top 15 Feature Importances:")
print(feature_importance_df.head(15))

# Plot top 15
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df["feature"].head(15), feature_importance_df["importance"].head(15))
plt.gca().invert_yaxis()
plt.title("Top 15 Churn Drivers")
plt.xlabel("Importance Score")

plot_path = os.path.join(OUTPUT_DIR, "feature_importance.png")
plt.savefig(plot_path)
plt.close()

print(f"\n‚úî Feature Importance chart saved: {plot_path}")
print("üéØ Step 4 Completed Successfully!")
```

‚Ä¢	Run the script;
```
python3 eval_04_feature_importance.py
``` 
<img width="975" height="529" alt="image" src="https://github.com/user-attachments/assets/d5579ae6-4be0-4e1b-8e9c-c019d229c77f" />



#### Step 5 ‚Äî SHAP Explainability
In this final step of model evaluation, we use SHAP explainability to reveal how each feature contributes to the model‚Äôs churn risk predictions, ensuring full transparency and interpretability for stakeholders and banking regulators.

‚Ä¢	In the evaluation folder, create a file named eval_05_shap_explainability.py and paste the below in it
```
# Step 5 ‚Äî SHAP Explainability (Bulletproof Version)

import os
import joblib
import shap
import matplotlib.pyplot as plt
import numpy as np
import scipy.sparse

print("\nüîç Phase 3 ‚Äî Step 5: SHAP Explainability")

ARTIFACT_DIR = "../../artifacts"
BEST_MODEL_PATH = os.path.join(ARTIFACT_DIR, "best_model.joblib")
DATA_PATH = os.path.join(ARTIFACT_DIR, "churn_ml_ready_data.joblib")
PIPELINE_PATH = os.path.join(ARTIFACT_DIR, "preprocess_pipeline.joblib")

OUTPUT_DIR = "../../evaluation_results"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Load components
model = joblib.load(BEST_MODEL_PATH)
data = joblib.load(DATA_PATH)
pipeline = joblib.load(PIPELINE_PATH)

X_test_processed = data["X_test"]

# Convert sparse to dense
if scipy.sparse.issparse(X_test_processed):
    X_test_dense = X_test_processed.toarray()
else:
    X_test_dense = X_test_processed

X_test_dense = X_test_dense.astype(float)

# Feature names
numeric_features = data["numeric_features"]
categorical_features = data["categorical_features"]
encoder = pipeline["preprocessor"].named_transformers_["cat"].named_steps["encoder"]
encoded_cat_features = encoder.get_feature_names_out(categorical_features).tolist()
feature_columns = numeric_features + encoded_cat_features

assert len(feature_columns) == X_test_dense.shape[1], \
    f"Feature mismatch: {len(feature_columns)} vs {X_test_dense.shape[1]}"

# SHAP Explainer
explainer = shap.TreeExplainer(model)
raw_shap_values = explainer.shap_values(X_test_dense)

# Pick correct SHAP output format
if isinstance(raw_shap_values, list):
    shap_values = raw_shap_values[-1]  # positive class
    print("üü¢ Using shap_values for binary classification")
else:
    shap_values = raw_shap_values
    print("üü¢ Using shap_values matrix directly")

# Summary plot
plt.title("SHAP Summary Plot - Churn Feature Impact")
shap.summary_plot(shap_values, X_test_dense,
                  feature_names=feature_columns, show=False)

summary_path = os.path.join(OUTPUT_DIR, "shap_summary.png")
plt.savefig(summary_path, bbox_inches="tight")
plt.close()

print(f"‚úî SHAP Summary saved: {summary_path}")
print("üéØ Phase 3 Completed Successfully!")
```

‚Ä¢	Run the script;
```
python3 eval_05_shap_explainability.py
``` 
<img width="975" height="224" alt="image" src="https://github.com/user-attachments/assets/605e33ca-03cb-40e8-9ab5-503ee216586c" />



Phase 4 ‚Äî Revenue Impact Analysis
While predicting customer churn provides the early warning signal, true business value is realized when those predictions are translated into financial impact and actionable retention strategy. Phase 4 focuses on quantifying the monetary risk associated with customer churn and highlighting high-value customers who require urgent intervention.
By combining churn probability scores with estimated Customer Lifetime Value (CLV), current balances, and salary inflow strength, we can accurately estimate the revenue at risk if a customer decides to leave. This enables bank leadership to prioritize retention activities where they will yield the highest return on investment.

This phase delivers clear and actionable insights such as:

i.	How much revenue is at risk in the customer portfolio

ii.	Which high-value customers are most likely to churn

iii.	The percentage of customers driving the majority of churn exposure

iv.	Data-driven recommendations for targeted retention strategies

Ultimately, Phase 4 transforms churn modeling from a technical exercise into a strategic business decision engine that directly protects profitability and strengthens long-term customer loyalty.

#### Step 1: Revenue Risk Calculation

We convert churn probability into estimated revenue loss by combining churn score with the financial value associated with each customer. This produces a new metric ‚Äî Revenue at Risk ‚Äî which quantifies how much money the bank may lose if that customer churns.

‚Ä¢	In the revenue_analysis folder, create a file named step_01_revenue_risk.py and paste the below in it
```
# Phase 4 ‚Äî Step 4.1: Revenue Risk Calculation

import sqlite3
import pandas as pd

print("\nüí∞ Phase 4 ‚Äî Step 4.1: Revenue Risk Calculation")

# Corrected database path (we are inside src/revenue_analysis folder)
DB_PATH = "../../database/customer_analytics.db"

# Connect to SQLite DB
conn = sqlite3.connect(DB_PATH)

# Load necessary columns from bank_customers table
query = """
SELECT customer_id, churn_probability, estimated_clv
FROM bank_customers
"""
df = pd.read_sql_query(query, conn)

# Calculate revenue risk: financial loss if customer churns
df["revenue_risk"] = df["churn_probability"] * df["estimated_clv"]

# Sort by highest financial exposure
df_sorted = df.sort_values(by="revenue_risk", ascending=False)

# Store results into revenue_insights table
df_sorted.to_sql("revenue_insights", conn, if_exists="replace", index=False)
conn.close()

# Display Top 10 high-risk customers for quick review
print("\nüîù Top 10 Customers With Highest Revenue Exposure (‚Ç¶):")
print(df_sorted.head(10))

print("\n‚úî Revenue Risk calculated and stored in revenue_insights table!")
print("‚û°Ô∏è Proceed to: Step 4.2 ‚Äî VIP Churner Prioritization")
```





‚Ä¢	Run the script;
```
python3 step_01_revenue_risk.py
``` 
<img width="975" height="377" alt="image" src="https://github.com/user-attachments/assets/0a469d84-64c3-4eb5-a96d-ae73973ffef5" />


#### Step 2: High-Value Churner Prioritization
In this step, we identify the segment of customers who simultaneously have high churn risk and high revenue value. These are the customers where targeted retention has the greatest financial impact.

‚Ä¢	In the revenue_analysis folder, create a file named step_02_vip_prioritization.py and paste the below in it
```
# Phase 4 ‚Äî Step 4.2: High-Value Churner Prioritization

import sqlite3
import pandas as pd

print("\nüéØ Phase 4 ‚Äî Step 4.2: High-Value Churner Prioritization")

DB_PATH = "../../database/customer_analytics.db"
conn = sqlite3.connect(DB_PATH)

df = pd.read_sql_query("SELECT * FROM revenue_insights", conn)

# Create prioritization tiers
def prioritize(row):
    if row["churn_probability"] >= 0.70 and row["estimated_clv"] >= 350000:
        return "VIP Critical"
    elif row["churn_probability"] >= 0.40 and row["estimated_clv"] >= 250000:
        return "VIP Concern"
    else:
        return "General Risk"

df["retention_priority"] = df.apply(prioritize, axis=1)

# Save updated table
df.to_sql("revenue_insights", conn, if_exists="replace", index=False)
conn.close()

print("\nüìå Priority Segments:")
print(df["retention_priority"].value_counts())

print("\n‚úî VIP Churn Prioritization Complete!")
print("‚û°Ô∏è Proceed to Step 4.3 ‚Äî Portfolio Risk Distribution")
```

‚Ä¢	Run the script;
```
python3 step_02_vip_prioritization.py
```
<img width="975" height="355" alt="image" src="https://github.com/user-attachments/assets/7f9d8648-04bd-476e-90a1-c84813937a0a" />


#### Step 3: Portfolio Risk Distribution

This step reveals how revenue risk is concentrated across customer segments. We quantify what percentage of customers generate the majority of churn exposure ‚Äî allowing the bank to focus resources efficiently.

‚Ä¢	In the revenue_analysis folder, create a file named step_03_portfolio_distribution.py and paste the below in it
```
# Phase 4 ‚Äî Step 4.3: Portfolio Risk Distribution

import sqlite3
import pandas as pd

print("\nüìä Phase 4 ‚Äî Step 4.3: Portfolio Risk Distribution")

DB_PATH = "../../database/customer_analytics.db"
conn = sqlite3.connect(DB_PATH)

df = pd.read_sql_query("SELECT * FROM revenue_insights", conn)

# Total Revenue at Risk
total_risk = df["revenue_risk"].sum()

# Contribution by priority segment
risk_by_segment = df.groupby("retention_priority")["revenue_risk"].sum()

print("\nüí∞ Total Revenue at Risk: ‚Ç¶{:,.2f}".format(total_risk))
print("\nüìå Revenue Risk Contribution by Tier:")
print(risk_by_segment)

# Contribution percentages
percent_by_segment = (risk_by_segment / total_risk) * 100

print("\nüìà Percentage Breakdown:")
print(percent_by_segment.round(2))

conn.close()

print("\n‚úî Portfolio Risk Distribution Report Generated!")
print("‚û°Ô∏è Proceed to Step 4.4 ‚Äî Business Recommendations")
```

‚Ä¢	Run the script;
```
python3 step_03_portfolio_distribution.py
```
<img width="975" height="526" alt="image" src="https://github.com/user-attachments/assets/e63d7041-0534-4370-a195-73bb490504be" />



#### Step 4: Business Recommendations
Based on churn drivers and financial exposure, we recommend proactive interventions focused on high-value customers, preventing churn before it occurs. Each recommendation is aligned with key churn reasons uncovered by the model.

‚Ä¢	In the revenue_analysis folder, create a file named step_04_recommendations.py and paste the below in it
```
# Phase 4 ‚Äî Step 4.4: Business Recommendations

import sqlite3
import pandas as pd

print("\nüß† Phase 4 ‚Äî Step 4.4: Business Recommendations")

DB_PATH = "../../database/customer_analytics.db"
conn = sqlite3.connect(DB_PATH)

df = pd.read_sql_query("SELECT * FROM revenue_insights", conn)

# Filter high value + high churn
vip_critical = df[df["retention_priority"] == "VIP Critical"].copy()

# Business rule mapping for recommendations
def recommend(row):
    if row["revenue_risk"] > 400000:
        return "üß≤ Personalized VIP retention package + Dedicated RM"
    elif row["churn_probability"] > 0.85:
        return "üìû Urgent outreach: Complaint resolution & product review"
    elif row["estimated_clv"] > 350000:
        return "üí≥ Offer premium product upgrade / loan restructuring"
    else:
        return "üìç Maintain digital engagement & loyalty incentives"

vip_critical["recommended_strategy"] = vip_critical.apply(recommend, axis=1)

print("\nüî• VIP Critical Retention Actions Needed:")
print(vip_critical[["customer_id", "revenue_risk", "recommended_strategy"]].head(10))

# Save recommendations to DB
vip_critical.to_sql("vip_recommendations", conn, if_exists="replace", index=False)

conn.close()

print("\n‚úî Business Recommendations Table Saved!")
print("‚û°Ô∏è PHASE 4 COMPLETED! üéâ")
```

‚Ä¢	Run the script;
```
python3 step_04_recommendations.py
``` 
<img width="975" height="482" alt="image" src="https://github.com/user-attachments/assets/d825fce3-a969-4889-bd20-57de0af8c9f7" />






### Step 5 ‚Äî Dashboard & Deployment Concept
In a real banking environment, artificial intelligence becomes truly valuable only when insights are delivered in a format that business leaders can understand and act upon immediately. After developing the churn prediction model and performing revenue impact analysis, the next crucial step is to operationalize these results in a centralized decision-support platform.
Phase 5 focuses on deploying an interactive AI-powered Churn & Revenue Risk Dashboard, designed specifically for Customer Retention Teams, Relationship Managers, and Senior Executives. This dashboard visualizes real-time churn probability, revenue exposure distribution, high-value customer risks, and recommended retention strategies generated by the AI model.
With streamlined filters, executive KPIs, VIP churn watchlists, and targeted outreach recommendations, the dashboard transforms complex analytics into clear, actionable intelligence. This enables the bank to act proactively‚Äîprioritizing the customers who matter most, reducing preventable churn, protecting Customer Lifetime Value, and enabling smarter resource allocation.
Ultimately, Phase 5 ensures that the advanced analytics built in earlier phases are not left on a data scientist‚Äôs laptop, but instead delivered as a live operational tool capable of supporting business decisions every single day.

‚Ä¢	Run these commands
```
cd /root/projects/bank_churn/src
mkdir -p dashboard
cd dashboard
touch app.py
```

‚Ä¢	Paste the below code in the just created app.py file
```
Code on app.py
```

‚Ä¢	Run the command;
```
streamlit run app.py --server.port 8505 --server.address 0.0.0.0
http://localhost:8505/
``` 
<img width="975" height="439" alt="image" src="https://github.com/user-attachments/assets/40b412c5-d3f0-442b-8d23-eb6ace8c3ac8" />




#### Now, let us enable Streamlit Dashboard to Auto-Run Using systemd

‚Ä¢	Create a systemd service file. Run;
```
sudo vi /etc/systemd/system/churn_dashboard.service
```

‚Ä¢	Paste this EXACT configuration (update the USER + paths to match your actual environment)
```
[Unit]
Description=Bank Churn Prediction Dashboard (Streamlit)
After=network.target

[Service]
User=root
WorkingDirectory=/root/projects/bank_churn/src/dashboard
ExecStart=/usr/local/bin/streamlit run app.py --server.port=8505 --server.address=0.0.0.0
Restart=always
RestartSec=10
Environment=PYTHONUNBUFFERED=1

[Install]
WantedBy=multi-user.target
```

‚Ä¢	Reload systemd to recognize the new service. Run;
```
sudo systemctl daemon-reload
```

‚Ä¢	Start the service now and enable startup at boot
```
sudo systemctl start churn_dashboard.service
sudo systemctl enable churn_dashboard.service
```

### Conclusion
The AI-Powered Bank Customer Churn Prediction & Revenue Impact Analysis System successfully delivers an end-to-end, enterprise-grade solution that transforms customer behavior data into proactive business action. By integrating machine learning predictions with advanced financial analytics and real-time visualization, this system empowers banks to protect revenue, enhance customer lifetime value, and optimize retention spending.
Through this project:
i.	We developed a reliable classification model that accurately identifies customers at risk of churn
ii.	We uncovered key behavioral and financial drivers of churn using explainable AI (Feature Importance & SHAP)
iii.	We quantified revenue at risk by merging churn probability with CLV and financial indicators
iv.	We segmented customers into actionable tiers ‚Äî ensuring high-value customers receive priority intervention
v.	We generated strategic retention recommendations tailored to financial exposure and churn drivers
vi.	We deployed an interactive dashboard for operational decision-making and performance monitoring
This solution moves the bank from reactive churn detection to proactive churn prevention, ensuring that customers are retained before they leave, and revenue is preserved rather than lost.

### Strategic Impact
The system delivers:
- Millions in potential revenue savings through targeted CLV retention
- Higher ROI for marketing and retention programs
- Improved customer satisfaction and loyalty
- Faster leadership decisions backed by real insights
- Future scalability to additional branches, products, and customers

It serves as a foundation for continuous improvement, allowing the bank to:
- Expand predictive analytics to loans, credit cards, and product cross-sell
- Integrate real-time behavioral signals into churn scoring
- Automate personalized customer outreach and campaign delivery

